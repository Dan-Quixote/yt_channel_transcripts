hey guys my name is Rob win and for the past couple of months I've been taking various large language model prompting courses so that you don't have to because today I'm going to be distilling everything that I learned into one simple easy to understand video with the most relevant bits I'm going to take you from zero beginner not even knowing the fundamentals of prompt engineering all the way up to ready to start iterating at with advanced level prompts in your production level application so without further Ado let's get into it oh and I just want to mention guys while I doing encourage you to take notes throughout this to really help with your learning I will include a free study guide that includes everything we cover here it'll be in the description below all right so first things first prompt engineering fundamentals what is it and why is it a thing so you can see here on the screen prompt engineering the definition is the practice of Designing inputs for ai ai ai tools that will produce optimal outputs and this goes for large language models it goes for image generation models goes for any sort of generative model where the better the prompt the better the output and so there's there's been this sort of Art and Science that's developed around making the prompts as good as possible to get the best outputs from the model that practice is known as prompt engineering and got a couple images on screen that I thought were interesting to take a look at first one go over on the right it's that funny uh it's all Ohio meme if you've seen it before on the internet and it's talking about how each progression as we've gone over the years and gone through the different eras of coding languages they've all been sort of abstractions up from the basic zeros and ones that's it's all eventually converted down to to actually communicate with the computer with the machine and it's been the Holy Grail The Dream the fantasy sort of since the beginning of computer science since the beginning of computer programming going all the way back to like those little Punch Cards they literally used to feed into the machine a to love lace and all that to be able to just tell the computer in plain language what you want done and what you want it to do that's been the fan that's been the dream and with these large language models we are sort of getting to that point you can see the tweet from Andre karpathy he's a famous computer scientist the hottest new programming language is English and that's because with these powerful models like chat gbt and others you can simply tell them a lot of times what you want done and it will be able to create a program with that functionality and not just creating computer programs like it will be able to do all sorts of things obviously it can generate images obviously you can analyze Excel spreadsheets create uh you know pie charts and graphs it can do all sorts of things that previously would have taken a very select skill set set to do it's not all the way there the functionality is pretty Limited at the moment it's it can do basic to inter intermediate stuff fairly well for the more advanced stuff usually a human will have to go in there and troubleshoot modify some things and sometimes of course it just gets it flat out wrong and you just have to do it on your own but we're slowly getting to that point in the key is with prompt engineering with a well-crafted prompt you can replace hundreds of lines code and in a lot of cases even the need to learn to code you don't really need to learn a programming language if you want to do fairly basic stuff or in the past even to do just simple things in Python you're probably still going to need to spend some time learning the syntax learning the basics like objects and classes and you know the various operators and in that sort of thing but with prompt engineering now with good prompts you can just tell the large language model to do it and it will take care and that's important because it frees up so much human energy to focus on other things to just continue this cycle of improvement and refinement and just continue the progress of Technology basically but you can see here on this screen in addition to replacing hundreds of lines of code a good prompt can also just vastly increase the accuracy of the responses you get then if you just fed it like a basic Google search type response it can also this is a big one effectively fine-tune your model to your specific use case so normally with fine-tuning you would have to go out uh get a bunch of data label a bunch of data validate it with you know either humans or other models more powerful models going over it and then you would have to train the model on it you'd have to test it evaluate it see how good it is that is a very timec consuming and expensive process and it's been found that with good prompts and specifically good examples that you can provide to the model that it can effectively fine-tune it pretty much it it can understand what you want done and it will tailor its responses to what you want and it can fine-tune it that way so a massive time save there with good prompting and then the last item there it can squeeze more juice out of the cheaper models like 3.5 chat gbt 3.5 for example and give it the performance of you know not obviously not fully the the latest model but it can improve it enough to where you can use the 3.5 in your applications and it's good enough to where the application makes sense and is scalable because the cheaper or the mo the latest models are quite expensive the earlier models a fraction of the cost so if you can squeeze those cheaper models and get them to perform you can save tons of money and your business or your application us use case can actually make Financial sense so that's another reason uh some use cases for large language models and the importance of prompt engineering within those use cases specific knowledge chat Bots and potentially voice agents with a well-crafted prompt you can utilize these for service tasks when people go to your website uh if they're trying to track their order if they want to return an item or track the return you can use these models and the inputs from the customer to essentially handle all that and just free up tons of Labor to focus on other projects you can see lead generation appointment setting all all these tasks that often times will require lots of human input for things that they just they can be automated fairly easily you can with good prompt engineering and with a suitable model you can just automate these and drastically reduce the cost cost for your business and for your clients on the right here is a picture of a NASA chatbot that Nasa uses it's called badara it is a like a biology and biomimicry specific chatbot and the NASA employees will interact with this chatbot as they go through their workflows and it's tailored specifically to the badara process it's this whole thing biomimicry is where you you model you model model structures and systems based on the patterns and anatomies you see in biology it's super interesting but you can see even Nasa uses prompt Engineering in this way to create like interactive chat Bots for their employees here we're going to go over just the types of large language models if you haven't seen these before it can be quite confusing to understand what's what what's a model what's a chatbot uh what what's open AI versus chat GPT Etc so hopefully this image will break it down at the top layer here you have the the models themselves these are like the code the weights all the the parameters of the neural network this whole massive just just data and GPT through open AI That's the model Gemini through Google that's the model and also the chat bot the name of the chatbot that you interact with Claude is the anthropic model that's also called Claude and then llama is the one that meta came out with which is open source the chatbots are a userfriendly wrapper on top of the model so that you don't have to interact with the model via code you can just type it in in your web browser and then it will send it via code to the model so the chatbot is chat gbt Bing Gemini those are just so you can type in it's super userfriendly and you don't have to worry about getting into all the code and interacting it with python or whatever and then there's task specific models which are fine-tuned like we mentioned earlier fine-tuned versions of Prior models where this one's GitHub co-pilot it's code specific Jasper has been fine-tuned for the copywriting task Amazon has built one that's another code whisper that's another code specific one there's all sorts of these for specific tasks that have been fine-tuned versions of the various models so that's sort of the lay of the land all right so the next thing we're going to go over the next fundamental is tokens what are tokens they're basic basically words the language model can't understand words it has to break them up into numbers or get the input as numbers so what the model will do is it will break up your input into words one token equals approximately 3/4 of a word and it will then convert those tokens into numbers and feed those numbers into the model it's pretty self-explanatory from this picture here the paragraph is broken up colorcoded into the different tokens you can see they are then converted into words for example this comic here the third entry in the sequence is token number 11 and you can see 11 at various points throughout the text here because there's multiple commas in this paragraph obviously there's tons and tons of different tokens that's why they go to up into the tens of thousands they go into into the hundreds of thousands and this is how the machine can understand your words by breaking into tokens now an important thing to notice is that less tokens is going to equal a cheaper system so in general you should always aim to make your prompts and your inputs as short as possible while still maintaining the accuracy and then also there's going to be a tradeoff potentially with the number of tokens versus the accuracy for example if you find that you can get 98% accuracy with 100 100 token prompt or you can get 100% accuracy with a th000 token prompt you have to sort of consider is that 2% difference in accuracy going to be worth those extra 900 tokens and all the costs that that are associated with it that'll depend on your use case you have to experiment see what you can tolerate but that's another thing to keep in mind next is going to be the system message the system me message is basically the North Star for the model it's always going to reference this whenever it's answering any prompt the you are helpful assistant here at the top that is the system message for chat gbt so whenever you input any prompt into chat gbt it first checks that system message says okay what who am I here what what's my role how should I frame this response what's the relationship between me and the input that sort of thing it's it's kind of funny you can go into the playground the open AI playground and modify this and say things like you are a massive jerk or like you're a total and it'll it'll formulate its responses to to reflect that and it will answer in a not so kind tone but the basic one is you are a helpful assistant this is similar to we'll go over later the role in Persona that you can input as part of the prompt framework uh but this is this is like one level above the role in Persona that you actually input in the prompt itself this is like embedded within the prompt and with chat GPT you can't even change this you can enter custom instructions which are again like a a sub system message but it's Main primary assistant message is always going to be you are a helpful assistant and uh this you are badara a biomic designer and research assistant that's actually the was the system message for the NASA chatbot that we saw earlier just explaining the role explaining what they're good at how they operate that sort of thing so you can get an idea of how it could be used in an actual application like that chatbot all right the next fundamental we over real quick this is temperature and this is essentially the randomness of the outputs that you get from the model the default is chat GPT or the default is one with chat GPT and what that means is when you put in the same output in the chat GPT you're not going to get the exact same output every single time it'll be slightly tweaked slightly randomized little bit different order a little bit different wording maybe a little bit difference a little bit of difference in the ideas that you get and that's usually what you want Chachi to to do to sort of come up with unique and creative ideas you can increase the randomness by increasing the temperature although if you go all the way up to level two what you'll find is you'll get is the extreme it's just like random gibberish It's So Random and then if you go all the way down to level zero it's actually the opposite it's completely deterministic and predictable it's always just going to pick what it deems the most probable next word in the sequence so you can imagine for for a production or business use case where you do want it to be S you want the same input to generate the same output if multiple customers at ask the same thing you wanted to give the same response if the same customer asks later what's you know the same question later you wanted to give it the same response so in those cases you would want to set that temperature down to zero to make it very or quite deterministic and then we have top P which will only consider words over certain percent percentage probability so you could set this to say 20% and it would only consider words that are over 20% for the next word so if the ne it has a whole list of words that it could be the next word but only dog cat and horse could be over 20% it says those are the only ones that have a 20% and we'll only consider those three words next three words in the sequence and generally you'll want to use either temperature or top PE in your apps you won't want to use both the next fundamental is delimiters you've probably seen these before delimiters really anything that breaks up text into a easierto read format even things like commas and periods and spaces and things like that are technically delimiters usually you'll see them in this markdown format with the number sign or asterics or underlines things like that delimiters are great because they can break up up the prompt so that it's much easier for you to read and generate and modify and also these models were trained on a lot of text that contained the limiters so they can understand them very well they can recognize sort of exactly what's going on when it sees them it's not going to confuse it so it's great for us it's good for the model it's just a win-win to use delimiters to break up your prompts and we'll see some examples a little bit later in the slideshow of how you can use these in prompts this this would be like a like a very basic template that you might have seen before in markdown format uh the next thing I want to go over is rag or retrieval augmented generation this is more of a like a technique but I wanted to put it in the fundamental section just because it's so it's become so ubiquitous over the past year or so to utilize this this retrieval augmented generation with models and what this does is it will go out out to some external Source find relevant information and then incorporate that information along with the user's query before it feeds it into a model so you've probably seen or been aware that jgpt has a knowledge cut off of a certain date this is a huge problem because if you're asking about things that require recent knowledge it's it can't really help all that much this is where rag comes in because what it can do is it can go out find relevant information via a tool via an interet internet search with Bing or Google get the relevant information that it needs combine that with the user prompt and then output give the output to the user doesn't have to be an internet search either can also be utilized with a knowledge base for let's say a specific company has a knowledge base that they want their model to pull from when it answers customer requests it will go to that knowledge base find the information that it needs needs and then answer the customer request with that knowledge it improves accuracy and reduces hallucinations previously if you asked it something about knowledge that it didn't have in its knowledge base it might just make something up obviously now that by giving it a specific knowledge base and the ability to go access that it's going to reduce that and then the final benefit is the knowledge base can up be updated and kept current over time and obviously internet searches can be kept current over time time as well and finally the last fundamental I want to go over is iterant iter iteration and experimentation prompt engineering is very much a Cutting Edge discipline we're still on the frontier here there's new techniques and new methods being discovered all the time most of the ones we'll go over in this presentation have been discovered in the last year or two super recent you might find something literally yourself that works better than anything else that's been discovered and start utilizing it and make it known make it make it a new technique that people start using the point is experimentation is key here you don't just want to go with the one prompt and call it a day you want to iterate see if it your prompts get better get worse what I'm going to teach you in this the rest of the slideshow is the framework and that's just the starting point you'll want to iterate from there try it out see how it works and eventually you'll sort of dial in on what techniques work best for your specific use case all right so now we are going to get into the meat and potatoes of this video The Prompt engineering framework and you can see on the left hand side there the six components of the framework the role the task the specifics the context examples and any final notes that you want to add now the framework is definitely just guidelines it's more what you call guidelines than actual rules you don't need to do every single part of the framework for every single prompt you don't need to put them in exact order you can combine them you can leave some out etc etc in fact the vast majority of prompts you've done in your p in the past if you use chat GPT probably haven't included this entire framework you just had one or two in there and it worked mostly just fine this is for you know getting really accurate resp respones you really want to dial it in and get the most the best outputs from your model in these production use cases for the more complex tasks like that you will want to abide by this framework and utilize it as far as building out your prompts like I said it's you can modify some things here and there but you will want to use it as a guide so first up roll SLP Persona we've seen it called both things this is where you simply tell the model what's going on just one to two sentences who is it what's it good at what's its role this is you are an expert YouTube video creator you specialize in making videos that are informative engaging fun interesting that sort of thing you are an expert car mechanic with 50 years of experience you specialize in working on foreign foreign makes and models that sort of thing you're telling the model what it's good at it's a world class expert at whatever your task is done and whatever specific attributes you want to have for your task that you'll want to include those key qualities as well if you want your videos to be professional you know clean cut very informative you'll say that if you want them to be funny engaging that's you'll want to include those here tell it what tell it what it's good at the next component is the task and this is basically just what you want the model to do this is the most basic element of any framework you pretty much need it in order for your prompt for the model to do anything or create any sort of output the task is the one you where you'll need to have it pretty much no matter what and everything is sort of built around that it's best to start with an action verb in these you know uh create generate analyze write things like that it tells the model specifically what you want done and as you can see there you want to be specific direct and succinct tell the model exactly what you want done use specific words but don't go overboard you don't want to be frivolous with your tokens both due to cost but also because using excess tokens in this area can actually confuse the model and it will try to do too much at once it won't be dialed into specifically what you want done so keep that in mind when you are building your task all right so now we'll go over the first technique of the presentation it's called Chain of Thought prompting and basically what it is is it's where you break down to the model exactly the thought process that you wanted to go through so so the image on the screen we have here is an actual excerpt from the paper that popularized this technique on the left you can see in the examples it gave the model it just said the answer is 11 but on the right with the example breaking down the math it gave it a step-by-step process proc for how to it came to each conclusion how it came to the answer and when the model was fed these two different examples the one on the left that just gives the answer couldn't quite reason it out it got the wrong answer whereas the one followed the stepbystep train of thought process exactly and it got the right answer in that case so we can utilize this in our prompts it can be utilized both here in the task section of the framework also potentially in the example example section when you're giving it uh the format and things like that you want it to follow you can also throw in some step-by-step processing as well there but it's important to just understand that when you break down the exact thought process you want the model to follow accuracy a lot of times can be boosted so it's a good technique to have in your Arsenal like you can see here it says or it's for tasks that require multi-step reasoning which a lot of the tasks you'll be using are more complex in the uh when you're you're doing for like a business application they're going to be more complex have multiple steps it's more much more effective in those types of situations it follows the format where you just want to say first step we're going to do this and then we get this with that we're going to do this and then finally do this etc etc like you saw in the previous slide now an interesting thing is doing that ideally or I mean breaking it down into the various steps might be ideal and might something you want to do depending on the use case but again token consideration is a thing and to limit tokens what you can consider is using this phrase let's think step by step at the end of your task this will tell the model or encourage the model to do this stepbystep analysis while also not needing to utilize the tokens of a much longer step-by-step prompt this has been found to get most of the the benefits of specifically you know detailing out each individual step you want so I would encourage you to try both out see which one works better see if one works much more than the other and see if it's worth those extra tokens or the extra work it takes to generate a step-by-step process for the model the next component of the framework is going to be the specifics section and this is where you're going to list out the most important notes for the task the additional details of the task so you can see an example here for the task is creating a script for a YouTube video in specifics you provide the details that you want between 3 or 5 minutes run time and to include at least one reference to the movie Pirates of the Caribbean whatever you want whatever for your your use case list format you usually works best for this section and you want to start small start with one or two items see how it goes you can add more if you need to uh if the model's not quite doing what you want again iteration is the theme here all right so the next item in the framework is going to be the context this is basically you want to make clear to the model what environment is it operating in what part of the business is it functioning in what type of customers is it going to be dealing with even things like what are the company values uh things that are important to the company stuff like that so it can better understand understand where it falls in the process better formulate its responses with specific language specific formats knowing what customer it's going to be talking to it's going to be talking to angry customers who uh are returning an item that didn't work or is it going to be talking to is it going to be people who are interested in the service and it's like a lead generation chat bot or that sort of thing giving it this context will better it will produce better responses uh for that all right now within the context portion of your prompt one of the techniques another technique you want to consider is this emotional stimuli otherwise known as emotion prompt these are phrases that stress the importance of the model's task they make it stress the importance of getting things right and you can see here on screen a bunch of examples taken directly from the paper that talked about this technique things like this is very important to my career things like you'd better be sure remember that progress is made one step at a time stay determined and keep moving forward other things you can use is this is vital to the business if you don't succeed in this task the business will fail things like that that really stress the importance of the task now why does this work you can see the bottom portion of the slide here this input attention attention is essentially how much how much Focus the model puts on particular words with the original prompt with no emotional stimuli this is the default the Baseline but when these emotional stimuli were added the model actually paid more attention to the important words in the previous sentence or in the task in this case which is just I think absolutely fascinating because it's it's exactly how a human would act obviously these models don't have emotions in the sense that humans do neurotransmitters and hormones and all that and yet when you give it an emotional stimulus like was saying this is so important to my career please please get it right it still does the exact same thing a human would do where it pays more attention to the task and it's it's essentially gets more focused on the task at hand which Super fascinating how that all works but the important thing for our framework is that by including this language you can get the model to pay even closer attention to the important parts of the task and therefore increase accuracy which obviously when you are prompt engineering accuracy is going to be a huge huge uh piece next part of the framework is going to be examples uh the first bullet point you might have seen these terms before zero shot one shot and few shot it's really simple zero shot is when you give a task to the model with no examples one shot is when you give a task to the model and it includes one example as far as how you want the format uh or the input and the output to look like and sound like that sort of thing and then F shot is simply two or more examples that's all it is it's very simple you'll see these show up in other contexts but that's all it means zero examples one example or more than one example examples are uh very important part of the framework because you can effectively fine-tune the model with really good examples this is known as in context learning we talked earlier about how fine-tuning can be very expensive but with examples good prompt engineering and in good examples you can essentially fine-tune the model for your specific use case and get it maybe not all the way as if it was truly fine-tuned but get it close enough to where you don't need to go through all that that laborious process of actually fine tuning the model you can also I skipped one here you can also uh specify you know the format the tone the length that you want the answers for for instance a lot of times when chpt will will produce an answer it will say you know the answer is 25 often times you'll just want it to say 25 with no additional text that's where you can use these examples to great effect to really dial in exactly how you want it to sound and look when with examples you will want to use ideally the toughest most ambiguous example because what that does is it allows the model to formulate its decision boundary if you're familiar with that term in machine learning where is the cut off for classifying one thing this way versus the other if you give it if you give it uh con like curveballs like confusing examples ambiguous ones and you give it the right answer that you want it will have a much better idea when it uh encounters those examples in the future but that being said if you don't have those examples examples readily available you can just give it the input and output format that you want often in just a Q&A format we'll see some examples of that when we when we actually look at the full framework in action but you can just get by with just using basic input and output that'll still tell exactly how you want the answer formatted it'll tell you exactly how like long it you want it to be that works just fine and then how many examples should you provide in your prompts the re Research indicates that going up to 20 there there's improvements and accuracy going all the way up to 20 examples and then it it really starts to level off but for most use cases both in a just a labor sense coming up with that many examples can be time intensive especially if you have if it's a longer like a more complex task even just coming up with one example might be quite take a long time and then obviously the tokens if you're providing 20 examples and the examples are quite long that's a massive amount of tokens for just one input and we already talked about how to more tokens equals more money so if we want these to be scalable if we want them to make Financial sense in production applications you want to limit the number of tokens so that being said usually three to five you can start there and see see the accuracy that you get 4 to8 is like the where you get the the best rip bang for your buck but I would start with 3 to five see how it does and you can you can add more if the the number of tokens your token constraints allow it and if you see are going to see noticeable increases again you you'll just have to iterate and see what you get but start with three work your way up and see see how it goes and then the last part of the framework is going to to be the final notes section this is where you reate reiterate the most important parts of the prompt and it's based on this uh this phenomenon known as The Lost in the-middle effect that has been found with large language models and also with human beings not surprisingly where we pay more attention to things that are at the beginning and the end of our input for instance with this presentation you probably pay you probably have better recollection and paay more attention to the very beginning and when we get to the end you'll probably have a better recollection of the things at the end all the things in the middle can get a little bit lost and machine learning or language models work the same way they can sort of lose information that's in the middle so this final note section is where you want to reiterate the most important things maybe in the task section in the specifics just to really get it clear to the model that these things are important and to make sure it doesn't forget this image here is actually uh from the research again you can see that this purple line is when the large language model was actually given the answers to the query in the prompt and information gets buried so heavily when it's lost in the middle that it actually performed worse when it was given the answer then when it was not given the answer when the answer was buried in the middle of the documents it was provided that just shows you how how big of an effect this loss in the- Middle effect is and like you can see here there's also uh other things like formatting notes just negative prompting that's where you tell the model to do not do this certain thing you want to reiterate it here quick note on negative prompting you will probably see better results if instead of saying do not do this you actually frame it in a positive way like refrain from doing this refraining tells it to it's it's a positive version of the Do Not verbiage they've seen slightly better results on that with the models all right so now we are just going to quickly go over what some example prompts look like full prompts when they're actually you know typed out not going to read all these to you can pause and compare if you want I I wanted to make them a little bit different with the names like this is role this is persona for the first component of the framework the delimiters are different the the list formats are different uh examples are this one doesn't really have any this one has the Q&A example section things like that to show you that there's a lot of flexibility with how you can con conru these prompts but this is the general framework you're want to shoot for and once you have this framework like this might be the first iteration of the prompt and then you'll see what response you get and then you can iterate from there so yeah feel free to pause the video go over these this is what a the full prompting framework looks like when it's all spelled out all right next I briefly wanted to go over some additional prompting techniques that you may not utilize but I think it's good to be aware of them for when you're looking out at the literature that's available or you're looking through other prompt engineering guides you have an idea you've encountered these before and you know what they are so we'll go over this image first on the left here we have our basic input output prompt which is just where you give a basic query or task to the model and it comes out with its output it's super basic it's probably what you've done tons of times already with the models to its right we have the up upgraded version of that the Chain of Thought prompting where you encourage the model to think step by step or you alternatively you can spell out the exact thought process that you want the model to take we've gone over that one as well next we haven't talked about this one this is a technique known as self-consistency with Chain of Thought So c-sc you'll see it formulated as and what this is basically is running Chain of Thought multiple times and then taking the majority vote of those results and that's your answer so it's certainly even with Chain of Thought prompting it's certainly possible that the mod model is going to come back with an incorrect result if you're just doing it one time to combat that self-consistency will run it multiple times and take the majority vote assuming that it's not going to get it wrong a majority of the time it's certainly possible but doing that even further reduces that chance and therefore incre increases consistency to its right on the other side of the dotted line is an even more complex technique known as tree of thought prompting and what this does is because Chain of Thought prompting and even self-consistency are known as greedy where they will just go to they'll take their current thought they'll go to the next one whatever they deem most appropriate they'll go to the next one by the time they get to the end even if there's no great Pathways forward from its prior thought it can't really backtrack and go back to the beginning and say okay we should probably take a different path here it'll just sort of continue on the path that is going and produce the best result result that it can even if it's not the optimal one tree of thoughts can combats this by encouraging the model to if you're familiar with uh data structures and algorithms if you've ever dabbled in that there's these things called breath first search and depth first search their search algorithms it's very similar to that or if you're not it's basically just going through a maze where it'll go down this particular pathway let's say these green ones represent valid or or good thoughts that it wants to keep pursuing it'll go down here but once it reaches this thought it will say okay there's not really any good next steps from this one let's go back to the beginning and try an alternative path just like you would if you were U exploring a maze or going you know like a a cave Explorer or something like that and then it'll try a different path it'll find a route through the it likes better than if it just with original Chain of Thought prompting once it got here because it was greedy it can't really go back it would have to pick one of these two suboptimal options and that would be your output with tree of thought prompting you can actually back up and encourage the model to try a different path until it finds one that it thinks is is better so that's tree of thought prompting that's an improvement on Chain of Thought prompting just like self-consistency was and then we got two more techniques that you'll encounter and use react prompting stands for it's short for reasoning and action prompting so if chain of thought prompting is encouraging model to go through its thought process react is an addition onto that by encouraging the model to in addition to explaining or going through its thought process going through its reasoning and its actions and what that looks like in practice is it encourages the model to not only explain its thoughts but explain the action it took and then what it now reasons about that new situation before moving on to the next stage in the process it's like thought action and then it's reasoning or observation about the new state of things and doing that can even further add on in uh accuracy to Chain of Thought prompting and then finally with prompts chaining this one you probably will use quite extensively in your applications and when you're using it for your business use cases because you can use the output from a prior prompt and use that output as the input for the next prompt in my future videos when I'm going over you know how to build chat Bots how to build voice agents things like that I will go over exactly how that is used in practice but just be aware that it's definitely something you can do where you sort of chain these prompts together and fit them all into one cohesive system and it's better to do it that way a lot of times because it can be much easier for a model to focus and perform one specific slightly narrow task and then take that output and put it into another slightly narrow task then to give it this huge input where it's just trying to do everything and then taking that output and it just gets confused with all those tokens with all these keeping track of all these different inputs a lot of times it's it's like um it's like if you're familiar with decoupling applications in general when building out your systems you can decouple the prompt to each prompt in output is a specific function you take that output put it into the new function it's it's that sort of thing so in summary just going to go over the key points here to make sure everyone's on the same page a good prompt can replace hours of work hundreds of lines of code and even negate the need for a larger better model and even negate the need for for knowing code knowing programming in a lot of cases it's great A lot of these a lot of these models and chat Bots and agents and uh now other things like that can all be built with no code tools just through good prompt engineering it's great uh again remember that prompt engineering is still cutting edge and everything is still being tinkered with being developed new techniques are constantly being discovered URI could be tinkering experiments doing our thing and discover a a whole new technique that that works great and better than anything else that's been discovered still very much the frontier here which is super exciting and the framework that we went over remember it's just a guideline you do not need to adhere to it 100% all the time just use it as a framework just use it as a guide for how to build your prompts and iterate iterate iterate said it several times now but yeah iteration is going to be key your first prompt is never going to be your last one in terms of accuracy you always want to experiment and see how you can get the most out of your prompts all right guys that is going to be it for this tutorial on prompt engineering I hop you learned something if you did please like this video feel free to share it if you know anybody who is interested in making their prompts better and subscribe to this channel if you're interested in more content I'll be coming out with a lot more regarding building out AI automation tools uh how to AI first your business chat Bots voice agents all that going to be doing it on this channel in the coming weeks and months so subscribe other than that thank you very much for watching you have a great rest of your day