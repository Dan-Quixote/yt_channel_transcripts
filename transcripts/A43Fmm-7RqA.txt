how's it going everybody my name is Rob win and this video is going to be about aspect-based sentiment analysis specifically how you can do it with the current as of May 2024 state-of-the-art language learning model instruct Absa now most of this video is going to be about how to use the instruct abso repo and how to get the model excuse me up and running so if that's what you're after feel free to skip ahead to the relevant part of the video but first I did just want to briefly go over ask ECT based sentiment analysis what it is why it's useful and how instruct Absa comes into play so we're all familiar with customer reviews for products we see these all over the Internet you can see some on your screen right now and they frequently have star ratings attached now General sentiment analysis which you've probably heard of would look at some text do its calculations and figure okay this piece of text seems like it's generally positive or generally negative and while that might be useful for some things it's not so useful for customer reviews because as we said reviews usually have star ratings attached and the star rating essentially tells us already what the overall sentiment is if it's four or five stars you can guess that the overall sentiment is going to be positive if it's one or two stars you can guess that the sentiment is probably going to be negative so what is a business owner what are they supposed to do if they want to get more detailed analysis on their product or service what if they want to know why people are giving positive or negative reviews that's where aspect-based sentiment analysis comes in because with this type of analysis as its name suggests you can dig into a review on a more granular level extract both the sentiment as well as the the specific aspect that is driving the sentiment so it won't just tell you this review seems positive it will say it will tell you these are the aspects that are driving that positive sentiment and also if there's negative sentiment in there it will tell you what aspects of your product or service are driving that negative sentiment so we can see on this Amazon page that Amazon has actually started doing this I want to say in the past year or so I'm not sure of the exact time frame but they have extracted various aspects of the product and assigned a sentiment classification uh to each aspect and that's what these little green check mark check marks mean if their model detected a trend of negative sentiment toward an aspect it will get a little orange negative there's none of that here with this product ones that don't have either a green check mark or a negative are considered neutral and this is what instruct Absa allows you to do so now I'm actually going to switch over to our hug hugging face space this is a space I created that hosts a fine-tuned version of the instruct Absa model you can enter in a sample review sentence you can and hit submit so we'll do that now and then it will return the extracted aspects as well as their Associated sentiments like this so you can see for this sample sentence food was extracted with an Associated positive sentiment the service was also extracted the other aspect with a negative sentiment and that matches the sample input sentence that we had here so that's the idea now obviously this would be very inefficient to go through let's say you had thousands of reviews or tens of thousands of review sentences and you wanted to run them through a model like this to try and figure out what aspects were good positive what aspects were negative it's obviously inefficient to do them one sentence at a time like this so that's where instruct abser comes in that's where now we're going to hop over to the code into the Jupiter notebook we'll take a look at it see how we can get it up and running and use it to perform the analysis for our business all right so here we are on the instruct Absa GitHub repo page just wanted to briefly take a look at this go over a few things just so everyone's on the same page so the main place we're going to be taking a look at in this video is this Joint Task training and inference notebook The Joint Task is the task where you give it a review or a review sentence and it will take a look at that sentence extract the aspects whether that's food whether that's the battery whether that's the service and then it assigns a sentiment to that aspect saying it's good or positive negative neutral based on the words around it that's the Joint Task there's also this at task which uh just extracts the aspect there's also an inference notebook this is where you have your model the default is just this instruct instruct Absa model not fine-tuned to your data and then you can speed it a sample sentence and it will spit out the results it's got a few different tasks here you can see uh going back a couple Pages we've got the instructions. piy which is a bunch of sets of instructions based on the given task that you're trying to do and it also has two sets for each task BOS instruct one and BOS instruct 2 BOS stands for beginning of sentence instruction that's what you feed to the model before you input your actual test that you want to translate basically giving it a prompt saying this is what I want you to do the reason there's two is because it was trained uh on both the laptop the laptop in the restaurant data sets there seems to be two standard data sets that they use for these types of tests the laptop reviews and then the restaurant reviews so there's different sets of instructions for each of those um here's the data sets folder this is where it's going to store the data sets for its tasks its test meaning instruct ABS as default tasks but when you clone this repo to your computer you can also store your own data sets in those folders just to make things a little easier so you don't have to modify too many file paths so you scroll down you can see uh just give some more information the big thing here is going to be this data set requirements it shows you how to how the format that you're supposed to have your data in so that the model can be trained effectively and there's a very specific format here as far as how you want it laid out a little bit later I'll actually show you a tool that I made to make this hopefully easier if you're going to be annotating your own data set sets uh just some more information here uh this instruct absit 2 for the Joint Task is somewhat relevant but I'll cover that when we go through the notebook and then the rest just more information if you want to run it from the CLI or something like that all right so here we are in The Joint Task H training and inference notebook that I referenced just a minute ago to get to this point you would clone the repo you would install the requirements.txt file and then you would launch up Jupiter notebook and get to this point that's assuming you want to run it on your local PC and not in collab which is also an option but I'll scroll down through this uh and just go over some of the things because there are going to be some problems if you try and run the cells in the notebook as is it's not completely updated so I wanted to show you real quick the changes you have to make to get it to work so just going to scroll down through this obviously you'll change your root path whatever you're working with I've changed mine uh then we'll go down to this training section you can change the experiment name to whatever you want and the model checkpoint you'll see I've updated to this Kevin scaria model that is on hugging face if you go back to the original notebook you'll see here that it's set as the Allen AI TK model that's the model that instruct abza was actually trained on but we want to fine-tune the instruct abza model does that make sense it was trained it was fine-tuned on Allen AI we want to fine-tune it so that's why we're updating the model checkpoint name and then the next cell these data sets the default is going to just be the laptop train and test you can update those to whatever you name your data sets probably best to just store them in the same folder so you don't have to change too much about the path and then when we try to run this cell as is let's say we update our data sets we have them to the past that we want and then we try to run this data set excuse me run this cell you'll see we get an error here it says the data set loader has no attribute create data in joint tesk format and this is where the notebook is a little bit at a date the default one you have to actually change this Joint Task in all its places you'll see that joint. joint you have to actually change that from joint or Joint Task to ASP so I'm going to do that real quick just to show you in real time how you do it instead of joint task you'll highlight it here and then it's going to be ASP that's assuming we're doing the still doing the Joint Task the aspect sentiment polarity and polarity extraction where we want both the aspect and the sentiment you'll change it in all the different spots here just like that now the other thing here you'll see in this comment it says at the beginning of the uh sentence instruction one for laptop and BOS instruct two for restaurant so I've already changed BOS to changed it to bos2 because my data set is a restaurant data set but you can change that to whatever is appropriate for you and also like it says you can modify the instructions. py file to include if you have a I don't know let's say you have a trucking business and you want to evaluate reviews for your trucking business you could set up some sample instructions there to get a little bit better accuracy so once we make those changes and then we go and run this set it runs fine and it'll move us to the next one uh this cell is all the training Arguments for the model when you go through your train hang on model checkpoint is not defined why are we getting that did we run this we did not run this that is why so now we're not getting an error but it's loading so we'll move on to this next one this is the actual train function and this is also where it's going to be a little bit out of date because I see I've already tried to run this cell and as it goes through the training process and you only have a train and a test set if you only have those two sets you'll actually get this error from the trainer function it's going to require an evaluation data set as you can see here and this is pretty frustrating because a lot of times you'll get decent ways into training which can take quite a long time I forget how long this took it took quite a while even though my sample sets are us really small in this case they're only like 20 samples long and still it took quite a while to get to this point because I'm running on a CPU only to get this error and tell me that I need the evaluation data set but anyway to avoid that error which we're going to do is split up your data set into not just a train and a test set but a train a validation and a test set which is pretty easy to do if you're comfortable with the train test split function but just for ease of everyone's use so you don't have to go and manually modify this what I actually did is I forked this repository and I made these changes in the notebook so if you want to do the Joint Task all you have to do is simply clone that repository and this notebook will be updated with all these changes automatically so you don't have to worry about getting everything perfectly right uh it has a section for splitting your data into train validation and test and likewise down here for this inference section we run into some of the same problems you'll see it's still at joint test down here so if you wanted to actually you know evaluate your model see how it did on the train set test it against maybe the Baseline instruct abza you would have to modify these cells as well all that's been done so that all you need to do is make sure you have the right instructions file and then obviously customize it with your own data sets and your own experiment name and file path so I'm going to take a quick look at that we'll switch right over I'll show you that real quick and then we'll look at that data annotation tool and wrap up all right so here we are in the updated notebook in in the new repository and I'll leave a link to this repository in the video description this is the fork repository that I made so I can make some changes to the notebook make it a little bit easier so everyone doesn't have to change everything manually if you scroll down you can see I've added some comments here to where you can enter your own details there's the file path your experiment name you can see the model checkpoint has already been updated so you don't have to change that can change your uh data set path and then the Joint Task has all been changed to ASP you'll still need to update the beginning of sentence instruction depending on what you need to do but that's just automatically set at two for restaurant we've got training arguments and then here's the big thing it's creating this validation set obviously it lets you know it's going to get an error if you don't create one and then it's got the code to actually create one so we'll use the train test split function to split the tokenized data set uh the test set into two and you can do whatever size you want we're just splitting it evenly into two half and half as you can see there and then in the next part because it's split now that this ID tokenized data set was split into two a train and test set we're just creating a test set with one half and a validation set set with the other half and when you print this out I haven't run this cell or anything but when you print this out you'll actually see how it's now divided into three separate parts you got the train you got the validation and you got the test and then I have tested this when you run the code you will no longer encount an error when you run this train function so that'll solve that problem it still will take a while probably depending on what you're running it on but you will no longer get the error and you'll be able to fine-tune your model and then going down below the inference has all been updated as well so all you need to do is update your data set as above change the instructions if you need to and run it and you'll be able to get the accurate output and compare your model versus a baseline compare different versions of your model and so on okay so the last thing I wanted to do was show you the data annotation tool this is a tool that you can use to annotate your reviews in the format that instruct abs and needs to perform the fine tuning on on them obviously to do that by hand for potentially hundreds of thousands of reviews would be extremely timec consuming so be nice to have a tool to make it a little bit quicker even though there's still not a great way to do it super quickly so it's just a little bit of manual labor but a tool like this definitely speeds it up so I'll show you the tool here this is just the code and Pie charm I will up load this to the GitHub repost so you have access to it but I wanted to quickly go over what's going on here so this first part here this is essentially taking your CSV file where you have all of your reviews just paste it into a column I'll show you here we go all these reviews each one is just in the a column its own separate row is its own review this is its review there's a duplicate that's what it looks like this is your data frame not your data frame this is your CSV file right here obviously mine's on my desktop you can have yours wherever this is going to point to just your CSV file with your reviews and what you do by running this block of code is it's going to create a new data frame and a new CSV file breaking each of those reviews into individual sentences using these four Loops it'll take each review so it'll take this review for instance which has four sentences and it will break up each sentence so that instead there's four separate rows each with one sentence a piece that's what this block of code does and you can see I've commented it here you only need to in you only need to do this one time and then you can comment this block of code out you won't need to run it again unless you're doing it with a new set of reviews but that's what this first section does and then when you actually run the code here's what the app looks like so I've got my tool here you can see this sentence here I loved the food so much corresponds with the very first sentence of the very first review that we're at the very top of the CSV file I love the food here so much but I have to say the service was lacking blah blah blah so it basically just breaks it all down into sentences and then what you can do from here is for each sentence you highlight any aspects that are in the sentence in this case food is an aspect and then you assign the sentiment to it so you highlight it with your cursor and then you click positive and you'll see we get the uh dictionary format down here with term polarity as the keys and then the specific aspect and the sentiment as the values and once you have that value loaded in there you'll hit next and it's going to take you to the next sentence you can see but I have to say the service was lacking and the food order process wasn't my favorite you can see that sentence now is loaded up in our annotation tool and you just do it again we've got an aspect service with negative sentiment food ordering process probably more accurate we'll say ordering process is an aspect they didn't like that was negative as well we'll hit next takes you to the next sentence and you just keep doing this for every sentence of every review so like I said it's not going to be blazing fast it's still a little bit of a manual process but it's a lot faster than literally typing that every single time uh it'll save you a ton of time on that front if there's no aspects in a particular sentiment you can just hit next and it'll basically just leave that row blank and that's fine too um but yeah you just keep doing this we can say Wings they're just okay so you would hit negative hit next and then here's a cool feature of this app let's say you know we got a pretty long CSV file with a lot of reviews let's say we've done it for a little wow we want to take a break what you can do if you just X Out of the app it will actually save your progress in whatever CSV file you have so if we go down to the save function we saved it to annotated sentences. CSV and we have that loaded up over here and we can see that the last sentence that we did which was this garlic parm wings that that's the last one that has any aspect terms loaded after it the rest are all just blank because we didn't reach those yet and so what you'll want to do to start back up since we've already done this step and converted our CSV file to this new data frame or CSV file of just sentences we don't have to do that again we can comment out this code and down here when it's asking us what's the CSV file we want to use to load up we'll actually use the one we just created in our case it's annotated sentences. CSV so we'll have to change that as well there's probably an easier way to do this but I just wrote this for myself and know I'm sharing it so if you want to modify the code to make it a little bit more uh intuitive for yourself feel free but now when we start it back up we're going to start where we were before so if we go back to annotated sentences after this bar garlic parm Wings sentence where we assign the sentiment the next was the wings were just okay that's where we start the wings were just okay neutral just okay blah blah we keep going we do this we can save it again it'll save our progress if we come back here and run it start starts back up where we left off so that's how you can more quickly go through and annotate the data for your instruct amum model and the last thing I wanted to go over was in addition to The annotation tool I also wrote up a couple of helper functions to clean up the annotated data set once you're all done with it as well as split it up into training test sets if you'd like to do that as well so once you've done gone through and annotated your entire data set you're going to have a bunch of terms like this one where it's just a blank with no aspect taken out of it so there's not there's basically no data from it this first function here the remove blank terms that's going to eliminate all of those rows and then save it into a new CSV file just to clean up the data a little bit so it's easier when you go in and make it into a data frame second one pretty self-explanatory it's just going to split the data into a train and test split you can adjust these however you want to it's 7030 by default and obviously you can change where you save the file so I'll go ahead and upload all these files that we've discussed on the GitHub that I'll link in the description below but this is basically the end of the quick tutorial of how to get instruct Absa up and running at least for the joint test for the most part even if you're trying to do some of the other tests TS this will probably be helpful I would think troubleshoot some of the stuff if you encounter any errors these were the problems that I encountered when trying to set up instruct abza for the first time because there's not a ton of documentation on it things get out of date and I just wanted to help out anyone else who might be trying to use this repo because it is a very very useful function I think and it can be used for a lot of good things uh create some really good business insights if use it appropriately so just wanted to help out anybody who was running into the same struggles that I was by all means if you have any questions please leave them in the comments below one thing that I didn't cover in this video that I might cover in a future video is how to actually scrape reviews off of Yelp and Google and that sort of thing to get data for your data set uh this sort all this tutorial basically assumes you already have that data but that can be a challenge as well to getting that so perhaps in the future I'll create a tutorial on how to do that but this is pretty much it for instruct Absa if I have any follow-up things that come up I'll post another video like I said if you have any questions leave them below I'll try to answer but thanks for watching you have a great rest of your day